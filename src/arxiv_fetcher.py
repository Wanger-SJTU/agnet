#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivè®ºæ–‡è·å–å™¨
è·å–å½“å¤©cs.AIé¢†åŸŸçš„æœ€æ–°è®ºæ–‡
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import time
import sys
from typing import List, Dict, Optional


class ArxivFetcher:
    """arXivè®ºæ–‡è·å–å™¨"""
    
    def __init__(self, base_url: str = "http://export.arxiv.org/api/query"):
        """
        åˆå§‹åŒ–arXivè·å–å™¨
        
        Args:
            base_url: arXiv APIåŸºç¡€URL
        """
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; ArxivFetcher/1.0)'
        })
    
    def get_recent_papers(self, category: str = "cs.AI", max_results: int = 50, days_back: int = 7) -> List[Dict]:
        """
        è·å–æœ€è¿‘å‡ å¤©æŒ‡å®šç±»åˆ«çš„æœ€æ–°è®ºæ–‡
        
        Args:
            category: è®ºæ–‡ç±»åˆ«ï¼Œé»˜è®¤ä¸ºcs.AI
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            days_back: å›æº¯å¤©æ•°ï¼Œé»˜è®¤ä¸º7å¤©
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        today = datetime.now().date()
        start_date = today - timedelta(days=days_back)
        
        # æ„å»ºæŸ¥è¯¢å‚æ•° - æŒ‰æäº¤æ—¥æœŸæ’åºè·å–æœ€æ–°è®ºæ–‡
        query = f"cat:{category}"
        
        params = {
            'search_query': query,
            'start': 0,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”ï¼Œè¿‡æ»¤æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
            papers = self._parse_xml_response(response.text, start_date, today)
            return papers
            
        except requests.RequestException as e:
            print(f"è¯·æ±‚arXiv APIå¤±è´¥: {e}")
            return []
        except ET.ParseError as e:
            print(f"è§£æXMLå“åº”å¤±è´¥: {e}")
            return []
    
    def _parse_xml_response(self, xml_content: str, start_date: datetime.date, end_date: datetime.date) -> List[Dict]:
        """
        è§£æarXiv APIçš„XMLå“åº”
        
        Args:
            xml_content: XMLå“åº”å†…å®¹
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è§£æåçš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # å‘½åç©ºé—´å¤„ç†
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', ns):
                paper = self._parse_paper_entry(entry, ns, start_date, end_date)
                if paper:
                    papers.append(paper)
                    
        except Exception as e:
            print(f"è§£æè®ºæ–‡æ¡ç›®å¤±è´¥: {e}")
            
        return papers
    
    def _parse_paper_entry(self, entry, ns, start_date: datetime.date, end_date: datetime.date) -> Optional[Dict]:
        """
        è§£æå•ä¸ªè®ºæ–‡æ¡ç›®
        
        Args:
            entry: XMLæ¡ç›®å…ƒç´ 
            ns: å‘½åç©ºé—´
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è®ºæ–‡ä¿¡æ¯å­—å…¸æˆ–None
        """
        try:
            # è·å–è®ºæ–‡ID
            id_elem = entry.find('atom:id', ns)
            if id_elem is None:
                return None
            paper_id = id_elem.text.split('/')[-1] if id_elem.text else None
            
            # è·å–æ ‡é¢˜
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None and title_elem.text else "æ— æ ‡é¢˜"
            
            # è·å–æ‘˜è¦
            summary_elem = entry.find('atom:summary', ns)
            summary = summary_elem.text.strip() if summary_elem is not None and summary_elem.text else "æ— æ‘˜è¦"
            
            # è·å–ä½œè€…
            authors = []
            for author_elem in entry.findall('atom:author/atom:name', ns):
                if author_elem.text:
                    authors.append(author_elem.text.strip())
            
            # è·å–æäº¤æ—¥æœŸ
            published_elem = entry.find('atom:published', ns)
            if published_elem is None or not published_elem.text:
                return None
                
            published_date = datetime.fromisoformat(published_elem.text.replace('Z', '+00:00')).date()
            
            # åªè¿”å›æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è®ºæ–‡
            if published_date < start_date or published_date > end_date:
                return None
            
            # è·å–åˆ†ç±»
            categories = []
            for category_elem in entry.findall('atom:category', ns):
                term = category_elem.get('term')
                if term:
                    categories.append(term)
            
            # è·å–PDFé“¾æ¥
            pdf_link = None
            for link_elem in entry.findall('atom:link', ns):
                if link_elem.get('title') == 'pdf' or link_elem.get('type') == 'application/pdf':
                    pdf_link = link_elem.get('href')
                    break
            
            return {
                'id': paper_id,
                'title': title,
                'authors': authors,
                'summary': summary,
                'published_date': published_date.isoformat(),
                'categories': categories,
                'pdf_link': pdf_link,
                'arxiv_url': f"https://arxiv.org/abs/{paper_id}" if paper_id else None
            }
            
        except Exception as e:
            print(f"è§£æè®ºæ–‡æ¡ç›®æ—¶å‡ºé”™: {e}")
            return None
    
    def format_papers_output(self, papers: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–è®ºæ–‡è¾“å‡º
        
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„è¾“å‡ºå­—ç¬¦ä¸²
        """
        if not papers:
            return "æœ€è¿‘7å¤©å†…æ²¡æœ‰æ‰¾åˆ°æ–°çš„cs.AIè®ºæ–‡ã€‚"
        
        output = []
        output.append(f"ğŸ“š arXiv cs.AI æœ€è¿‘7å¤©æœ€æ–°è®ºæ–‡ ({len(papers)}ç¯‡)")
        output.append("=" * 60)
        
        for i, paper in enumerate(papers, 1):
            output.append(f"\n{i}. {paper['title']}")
            output.append(f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'ç­‰' if len(paper['authors']) > 3 else ''}")
            output.append(f"   è®ºæ–‡ID: {paper['id']}")
            output.append(f"   æäº¤æ—¥æœŸ: {paper['published_date']}")
            output.append(f"   åˆ†ç±»: {', '.join(paper['categories'])}")
            output.append(f"   PDFé“¾æ¥: {paper['pdf_link']}")
            html_path = paper['arxiv_url'].replace("abs", "html")
            output.append(f"  htmlé“¾æ¥: {html_path}")
            output.append(f"   arXivé¡µé¢: {paper['arxiv_url']}")
            
            # æ‘˜è¦å‰100ä¸ªå­—ç¬¦
            summary_preview = paper['summary'][:100] + "..." if len(paper['summary']) > 100 else paper['summary']
            output.append(f"   æ‘˜è¦: {summary_preview}")
            output.append("-" * 40)
        
        return "\n".join(output)


