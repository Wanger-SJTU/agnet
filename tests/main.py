
from datetime import datetime


from src.agent import LLMAgent
from src.config_loader import ConfigLoader
from src.arxiv_fetcher import ArxivFetcher
from src.html2md import html2markdown

# llm = LLMAgent(provider="deepseek", config_path="/home/wanger/codes/arxiv_agent/config/config.yaml")
llm = LLMAgent(provider="alibaba", config_path="/home/wanger/codes/arxiv_agent/config/config.yaml")

print(llm.ask("ä½ æ˜¯è°"))
print()
"""ä¸»å‡½æ•°"""
print("ğŸš€ å¼€å§‹è·å–arXiv cs.AIæœ€æ–°è®ºæ–‡...")

fetcher = ArxivFetcher()

# è·å–æœ€è¿‘7å¤©çš„è®ºæ–‡
papers = fetcher.get_recent_papers(category="cs.AI", max_results=50, days_back=10)

for paper in papers:
    html_path = paper['arxiv_url'].replace("abs", "html")
    
    # è·å–htmlpathå¯¹åº”é“¾æ¥å†…å®¹
    try:
        import requests
        
        # å‘é€HTTPè¯·æ±‚è·å–HTMLå†…å®¹
        response = requests.get(html_path)
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        
        # è·å–HTMLå†…å®¹
        html_content = response.text
        
        # æ‰“å°æˆåŠŸä¿¡æ¯
        print(f"æˆåŠŸè·å–HTMLå†…å®¹ï¼Œé“¾æ¥: {html_path}")
        print(f"å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
        print(llm.ask(f" {html2markdown(html_path)} ç»™å‡ºè®ºæ–‡çš„10ä¸ªå…³é”®è¯"))
        exit(0)
    except requests.RequestException as e:
        print(f"è·å– {html_path} å¤±è´¥: {e}")
